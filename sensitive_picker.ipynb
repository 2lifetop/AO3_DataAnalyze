{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba #分词\n",
    "from wordcloud import WordCloud #词云\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['font.family'] = 'simhei'\n",
    "import random\n",
    "import pandas as pd\n",
    "#from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sens_words_path = 'sensitive_words.txt'\n",
    "# sens_words = read_words_list(sens_words_path)\n",
    "# sens_words = list(set(sens_words))\n",
    "# sens_words.sort()\n",
    "# sens_words_str = \"\\n\".join(sens_words)\n",
    "# fh = open('sensitive_words.txt','w')\n",
    "# fh.write(sens_words_str)\n",
    "# fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取词表\n",
    "def read_words_list(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    strlist = []\n",
    "    for l in lines:\n",
    "        if '#' != l[0] and '' != l.strip():\n",
    "            l = l.strip()\n",
    "            strlist.append(l)\n",
    "    return strlist\n",
    "\n",
    "#查找敏感词\n",
    "def check_sens_words(text, sens_words):\n",
    "    ttext = text.strip()\n",
    "    sw_dict = {} #敏感词\n",
    "    for sw in sens_words:\n",
    "        n = ttext.count(sw) #敏感词出现次数\n",
    "        if n>0:\n",
    "            if not sw_dict.__contains__(sw):\n",
    "                sw_dict[sw] = 0\n",
    "            sw_dict[sw] += n\n",
    "    return sw_dict\n",
    "\n",
    "def sentence_cut(text):\n",
    "    pattern = pattern = r'\\.|/|;|\\?|!|。|；|！|……|\\n'\n",
    "    pre_sentence = re.split(pattern, text)\n",
    "    sentence = []\n",
    "    for pre in pre_sentence:\n",
    "        if len(pre) > 8:\n",
    "            sentence.append(pre)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取ao3文章\n",
    "def extract_ao3_work(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #提取分级标签\n",
    "    rating_dd = soup.find('dd', attrs={'class': 'rating tags'}) #找到分级标签\n",
    "    rating_a = rating_dd.find('a', attrs={'class': 'tag'}) #找到对应的a标签\n",
    "    rating = rating_a.string #获得标签文字\n",
    "    \n",
    "    stats_dd = soup.find('dl', attrs={'class': 'stats'})\n",
    "    #提取点击量\n",
    "    hits_dd = stats_dd.find('dd', attrs={'class': 'hits'}) #找到分级标签\n",
    "    try:\n",
    "        hits = int(hits_dd.string)\n",
    "    except AttributeError:\n",
    "        hits = 0\n",
    "    \n",
    "    #提取发布日期\n",
    "    published_dd = stats_dd.find('dd', attrs={'class': 'published'}) #找到分级标签\n",
    "    date_str = published_dd.string\n",
    "\n",
    "    #提取文章\n",
    "    article_div = soup.find('div', attrs={'role': 'article'}) #找到文章标签\n",
    "    article_userstuff = article_div.find('div', attrs={'class': 'userstuff'})\n",
    "    article_str = str(article_userstuff)\n",
    "    article_str = article_str.replace(\"<br>\",\"\")\n",
    "    article_str = article_str.replace(\"<p>\",\"\")\n",
    "    article_str = re.sub(\"<div[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*[ \\>\\n]\",\"\",article_str)\n",
    "    article_str = re.sub(\"<p[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*[ \\>\\n]\",\"\",article_str)\n",
    "    article_str = re.sub(\"<br[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*[ \\>\\n]\",\"\",article_str)\n",
    "    article_str = article_str.replace(\"</p>\",\"。\")\n",
    "    article_str = article_str.replace(\"</br>\",\"\\n\")\n",
    "    article_str = article_str.replace(\"&nbsp;\",\" \")\n",
    "    article_str = article_str.replace(\"</div>\",\"\")\n",
    "    article_str = re.sub(\"<b[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*[ \\>\\n]\",\"\",article_str)\n",
    "    article_str = article_str.replace(\"<\\b>\",\"\")\n",
    "    article_str = re.sub(\"<img[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*[ \\>\\n]\",\"\",article_str)\n",
    "    article_str = article_str.replace(\"> \",\"\")\n",
    "    article_str = article_str.replace(\"<br\",\"\")\n",
    "    article_str = article_str.replace(\"<em\",\"\") #漏网\n",
    "    article_str = re.sub(\"[a-z\\ 0-9=\\\"\\',\\[\\]\\-\\_]*\\\"\\>\",\"\",article_str)\n",
    "    \n",
    "    # print(article_str)\n",
    "    # time.sleep(3)\n",
    "    return rating, hits, date_str, article_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_words_path = 'sensitive_words.txt'\n",
    "sens_words = read_words_list(sens_words_path)\n",
    "base_path = \"fulltext/\"\n",
    "sex_path = \"train/sex/\"\n",
    "pos_path = \"train/pos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ao3_pbar = tqdm(os.listdir(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬sex句子\n",
    "for work in ao3_pbar:\n",
    "    work_path = os.path.join(base_path,work)\n",
    "    with open(work_path) as f:\n",
    "        work_str = f.read() #读取文章\n",
    "    rating, hits, date_str, article_str = extract_ao3_work(work_str)\n",
    "#     #不要低分级文章\n",
    "#     if \"General Audiences\" in rating or \"Teen And Up Audiences\" in rating:\n",
    "#         continue\n",
    "#     ao3_pbar.set_description(\"{} | {}\".format(work, rating))\n",
    "#     print(work, rating)\n",
    "#     print('-'*40)\n",
    "    sents = sentence_cut(article_str)\n",
    "    for it,sent in enumerate(sents):\n",
    "        sens_pkg = check_sens_words(sent, sens_words)\n",
    "        if sens_pkg:\n",
    "#             sents_buf = [sent]\n",
    "            sum_sens = 0 #\n",
    "            for spv in sens_pkg.values(): #统计词汇数量\n",
    "                sum_sens += spv\n",
    "            prev_sum_sens = sum_sens-2\n",
    "            #增加涉及敏感词的句子量\n",
    "            final_subi = 0\n",
    "            stop_flg = False\n",
    "            for subi in range(1,random.randint(2,6)):\n",
    "                final_subi = subi\n",
    "                if prev_sum_sens > sum_sens-2: #增加量小于2\n",
    "                    stop_flg = True\n",
    "                    break\n",
    "                prev_sum_sens = sum_sens\n",
    "                if it-subi>=0:\n",
    "                    sent = sents[it-subi]+ \"。\" + sent\n",
    "#                     sents_buf.append(sents[it-subi]) #加入句子\n",
    "                if it+subi<len(sents):\n",
    "                    sent += \"。\" + sents[it+subi]\n",
    "#                     sents_buf.append(sents[it+subi]) #加入句子\n",
    "                sens_pkg = check_sens_words(sent, sens_words) #检测敏感词\n",
    "                sum_sens = 0\n",
    "                for spv in sens_pkg.values(): #统计词汇数量\n",
    "                    sum_sens += spv\n",
    "            \n",
    "#             #果然敏感词很多，则包含前后普通句子\n",
    "#             if sum_sens>5:\n",
    "#                 if not stop_flg:\n",
    "#                     final_subi += 1\n",
    "#                 if it-final_subi>=0:\n",
    "# #                     sent = sents[it-final_subi]+ \"。\" + sent\n",
    "#                     sents_buf.append(sents[it-final_subi]) #加入句子\n",
    "#                 if it+final_subi<len(sents):\n",
    "# #                     sent += \"。\" + sents[it+final_subi]\n",
    "#                     sents_buf.append(sents[it+final_subi]) #加入句子\n",
    "#                 final_subi += 1\n",
    "            if sum_sens>1:\n",
    "#                 print(sent, sens_pkg, final_subi)\n",
    "                sent_path = os.path.join(sex_path, work.replace('.html',\"_\")+str(it)+'.txt')\n",
    "                fh = open(sent_path,'w')\n",
    "                fh.write(sent)\n",
    "                fh.close()\n",
    "#                 for t,s in enumerate(sents_buf):\n",
    "#                     sent_path = os.path.join(sex_path, work.replace('.html',\"_\")+str(it) +\"_\"+str(t)+'.txt')\n",
    "#                     fh = open(sent_path,'w')\n",
    "#                     fh.write(s)\n",
    "#                     fh.close()\n",
    "#     print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中文比例\n",
    "def str_ratio_zh(instr):\n",
    "    cnt = 0\n",
    "    for s in instr:\n",
    "        # 中文字符范围\n",
    "        if '\\u4e00' <= s <= '\\u9fa5':\n",
    "            cnt += 1\n",
    "    return cnt/len(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#爬pos句子\n",
    "for work in ao3_pbar:\n",
    "    work_path = os.path.join(base_path,work)\n",
    "    with open(work_path) as f:\n",
    "        work_str = f.read() #读取文章\n",
    "    rating, hits, date_str, article_str = extract_ao3_work(work_str)\n",
    "    #要低分级文章\n",
    "    skip_rate = 40\n",
    "    if \"General Audiences\" not in rating and \"Teen And Up Audiences\" not in rating:\n",
    "        skip_rate = 60\n",
    "#     ao3_pbar.set_description(\"{} | {}\".format(work, rating))\n",
    "#     print(work, rating)\n",
    "#     print('-'*40)\n",
    "    sents = sentence_cut(article_str)\n",
    "    final_it = -1\n",
    "    for it,sent in enumerate(sents):\n",
    "        #抛弃一定比例的句子\n",
    "        if it < 5 or random.randint(0,100) < skip_rate or it<final_it or str_ratio_zh(sents)<0.8 or it > len(sents)-5:\n",
    "            continue\n",
    "            \n",
    "        sens_break_flg = True\n",
    "        sens_pkg = check_sens_words(sent, sens_words)\n",
    "        if not sens_pkg:\n",
    "#             sents_buf = [sent]\n",
    "            sens_break_flg = False\n",
    "            #增加涉及敏感词的句子量\n",
    "            for subi in range(1,random.randint(2,6)):\n",
    "                if it-subi>=0:\n",
    "                    sent = sents[it-subi]+ \"。\" + sent\n",
    "#                     sents_buf.append(sents[it-subi])\n",
    "                if it+subi<len(sents):\n",
    "                    sent += \"。\" + sents[it+subi]\n",
    "#                     sents_buf.append(sents[it+subi])\n",
    "                    final_it = it+subi\n",
    "                    \n",
    "                sens_pkg = check_sens_words(sent, sens_words) #检测敏感词\n",
    "                if sens_pkg:\n",
    "                    sens_break_flg = True\n",
    "                    break\n",
    "            if not sens_break_flg: #不涉sex文抛弃\n",
    "                #print(sent, sens_pkg, final_subi)\n",
    "                sent_path = os.path.join(pos_path, work.replace('.html',\"_\")+str(it)+'.txt')\n",
    "                fh = open(sent_path,'w')\n",
    "                fh.write(sent)\n",
    "                fh.close()\n",
    "#                 for t,s in enumerate(sents_buf):\n",
    "#                     sent_path = os.path.join(pos_path, work.replace('.html',\"_\")+str(it) +\"_\"+str(t)+'.txt')\n",
    "#                     fh = open(sent_path,'w')\n",
    "#                     fh.write(s)\n",
    "#                     fh.close()\n",
    "            else:\n",
    "                final_it += 3\n",
    "            final_it += 1\n",
    "#     print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"train/ao3.csv\"\n",
    "csv_train_path = \"train/train.csv\"\n",
    "csv_test_path = \"train/test.csv\"\n",
    "#label content\n",
    "ao3sents = []\n",
    "\n",
    "train_num = 78000\n",
    "max_pos_num = 125450\n",
    "\n",
    "sex_train_pbar = tqdm(os.listdir(sex_path)[:train_num])\n",
    "pos_train_pbar = tqdm(os.listdir(pos_path)[:int(1.025*train_num)])\n",
    "\n",
    "sex_test_pbar = tqdm(os.listdir(sex_path)[train_num:])\n",
    "pos_test_pbar = tqdm(os.listdir(pos_path)[int(1.03*train_num):])\n",
    "\n",
    "with open(csv_train_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for sex in sex_train_pbar:\n",
    "        work_path = os.path.join(sex_path,sex)\n",
    "        with open(work_path) as fh:\n",
    "            sent = fh.read()\n",
    "        writer.writerow([1, sent])\n",
    "    for pos in pos_train_pbar:\n",
    "        work_path = os.path.join(pos_path,pos)\n",
    "        with open(work_path) as fh:\n",
    "            sent = fh.read()\n",
    "        writer.writerow([0, sent])\n",
    "        \n",
    "with open(csv_test_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for sex in sex_test_pbar:\n",
    "        work_path = os.path.join(sex_path,sex)\n",
    "        with open(work_path) as fh:\n",
    "            sent = fh.read()\n",
    "        writer.writerow([1, sent])\n",
    "    for pos in pos_test_pbar:\n",
    "        work_path = os.path.join(pos_path,pos)\n",
    "        with open(work_path) as fh:\n",
    "            sent = fh.read()\n",
    "        writer.writerow([0, sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
