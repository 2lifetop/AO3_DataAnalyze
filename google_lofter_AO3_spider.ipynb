{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AO3文章下载器\n",
    "\n",
    "和大家之前说过，相关代码我会放出来，现在代码已经上传了去Github搜索 czw90130/AO3_DataAnalyze，我也和大家讲解一下实现方法，希望能激发大家学习编程兴趣。\n",
    "\n",
    "这里先讲解爬虫是怎么实现的，至于文本分析器。下周有空再写吧，实在是肝不动了！\n",
    "\n",
    "先说一下我的配置环境：\n",
    "* 操作系统：Ubuntu 18.04\n",
    "* Python环境：conda 4.8.2 Python 3.7.6\n",
    "* 编辑器使用的是 Jupyter Notebook 代码也是ipynb格式。想学python或者数据分析的可以用这个，适合组织文章。\n",
    "\n",
    "使用到的 Python 库有：\n",
    "* sys os time 不解释了\n",
    "* re 正则表达式解析\n",
    "* tqdm 进度条\n",
    "* selenuim 自动化测试/爬虫\n",
    "* BeautifulSoup html 标签解析器\n",
    "\n",
    "文章分析器使用的库也在这里说一下：\n",
    "* jieba 结巴中文分词，就是吧文章拆分成一个个词语\n",
    "* wordcloud 词云生成器\n",
    "* matplotlib 图表绘制库\n",
    "* numpy python数学运算库（这个其实就是做了个计数器～）\n",
    "\n",
    "都是非常常用的库，对 Python 和数据分析有兴趣的朋友可以照着这个表看看。（操作系统不熟Windows也可以）\n",
    "对AO3的爬取并不复杂，但还是有一些难度的。经过我一系列的测试发现,Request并不能有效的爬取AO3的信息，因此就选择了selenium。对于Selenium的介绍，我就过多不多描述了，有很多技术文介绍。大家可以自行搜索。我这里给大家一些关键词，方便大家搜索。\n",
    "\n",
    "* selenium配置chromedriver\n",
    "* selenium元素定位方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取AO3的中文文章其实并不复杂，\n",
    "\n",
    " 1. 我们进入AO3首页，点击 “Search” 然后点击 “Edit Your Search” 进入高级搜索模式\n",
    " ![进入搜索](./img/search.jpg)\n",
    " 2. 在Language里选择中文，点击“Search”\n",
    " ![高级搜索](./img/gaojisearch.jpg)\n",
    " 3. 把滚动条拉倒最下面，点第二页\n",
    " 4. 把浏览起里面的地址复制下来\n",
    "  ![第二页](./img/page2.jpg)\n",
    "\n",
    " 我们仔细查看这个url请求，发现这个请求的参数还是非常清晰的，让我们来看看works/后面的参数：\n",
    " \n",
    " `search?commit=Search&page=`后面跟着一个数字2，我们点击其第三页，这个数字也变成了3。所以可以断定这个参数指的是页码  \n",
    " `&work_search%5Blanguage_id%5D=` 后面跟着zh字样，可以判这个参数是控制语言的  \n",
    " 同理`&work_search%5Brating_ids%5D=`控制的是分级  \n",
    " \n",
    " 其他字段也是类似的，大家有兴趣可以自己试验，我就不再叙述了，值得一提的是，我在爬取时并没有用到分级标签功能，只是在搜索里面翻文章。通过多次搜索我发现AO3的搜索结果有一定的随机性，并没有主动干预搜索结果，这一点还是很良心的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获谷歌取搜索页面\n",
    "def make_google_search_url(page=0, num=100):\n",
    "    base_loc = 'https://www.google.com/search?hl=en&q=site:lofter.com+link:archiveofourown&safe=images'\n",
    "    base_loc += \"&num=\"+str(num)\n",
    "    base_loc += \"&start=\"+str(page*num) #搜索页\n",
    "\n",
    "    return base_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从谷歌获取文章链接\n",
    "def get_url_from_search(html):\n",
    "    old_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    search_div = soup.find('div', attrs={'id': 'search'})\n",
    "    div_g_groups = search_div.findAll('div', attrs={'class': 'g'})\n",
    "    for g in div_g_groups:\n",
    "        div_r = g.find('div', attrs={'class': 'r'})\n",
    "        a_hurl = div_r.find('a')\n",
    "        old_list.append(a_hurl['href'])\n",
    "    \n",
    "    return old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ao3_from_lofter(lofter_url_list, browser, path):\n",
    "    for url in lofter_url_list:\n",
    "        print(url)\n",
    "        dir_name = url.replace(\"http://\",'').replace(\".com/\",'_').replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "        dir_path = os.path.join(path,dir_name)\n",
    "        isExists=os.path.exists(dir_path)\n",
    "        if isExists:\n",
    "            print(Exists)\n",
    "            continue\n",
    "        # 判断结果\n",
    "        ao3_links = []\n",
    "        browser.get(url)\n",
    "        currurl = browser.current_url\n",
    "        if \"archiveofourown.org/works\" in currurl or \"archiveofourown.com/works\" in currurl: #如果url 直接跳转\n",
    "            ao3_links.append(currurl)\n",
    "            lhtml = \"\"\n",
    "        else: #没有跳转\n",
    "            lhtml = browser.page_source\n",
    "            soup = BeautifulSoup(lhtml, 'html.parser')\n",
    "            alink_groups = soup.findAll('a', attrs={'rel': 'nofollow'})\n",
    "            for alink in alink_groups:\n",
    "                href_str = alink['href']\n",
    "                if \"archiveofourown.org/works\" in href_str or \"archiveofourown.com/works\" in href_str:\n",
    "                    ao3_links.append(href_str)\n",
    "        \n",
    "        if ao3_links:\n",
    "            # 判断路径是否存在\n",
    "            isExists=os.path.exists(dir_path)\n",
    "\n",
    "            # 如果不存在则创建目录\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "            links_str = url + '\\n'\n",
    "            \n",
    "            need_agree = True\n",
    "            for work_url in ao3_links: #遍历ao3链接\n",
    "                links_str += work_url + '\\n'\n",
    "                \n",
    "            print(os.path.join(dir_path,\"links.txt\"))  \n",
    "            fh = open(os.path.join(dir_path,\"links.txt\"), 'w') #保存页面\n",
    "            fh.write(links_str) #写入内容\n",
    "            fh.close() #关闭\n",
    "            \n",
    "            print(os.path.join(dir_path,\"lofter.html\"))  \n",
    "            fh = open(os.path.join(dir_path,\"lofter.html\"), 'w') #保存页面\n",
    "            fh.write(lhtml) #写入内容\n",
    "            fh.close() #关闭\n",
    "            for work_url in ao3_links:\n",
    "                browser.get(work_url)\n",
    "\n",
    "                if need_agree:\n",
    "                    try:\n",
    "                        time.sleep(3)\n",
    "                        browser.find_element_by_id('tos_agree').click()\n",
    "                        time.sleep(1)\n",
    "                        browser.find_element_by_id('accept_tos').click()\n",
    "                        time.sleep(1)\n",
    "\n",
    "                        need_agree = False\n",
    "                    except NoSuchElementException:\n",
    "                        need_agree = False\n",
    "\n",
    "                work_html_text = browser.page_source  #获得页面代码\n",
    "                if \"If you accept cookies from our site and you choose \\\"Proceed\\\"\" in work_html_text: #无法获取正文则点击Proceed\n",
    "                    browser.find_element_by_link_text('Proceed').click()\n",
    "                    time.sleep(1)\n",
    "                    browser.get(work_url)\n",
    "                    work_html_text = browser.page_source\n",
    "                if \"Retry later\" in work_html_text:\n",
    "                    while \"Retry later\" in work_html_text:\n",
    "                        print(work_path)\n",
    "                        fh = open(work_path, 'w') #保存页面\n",
    "                        fh.write(\"Need_to_reload\") #写入内容\n",
    "                        fh.close() #关闭\n",
    "                        print(e)\n",
    "                        time.sleep(3)\n",
    "                        browser.get(\"http://www.baidu.com\")\n",
    "                        time.sleep(3)\n",
    "                        browser.quit()\n",
    "                        c_service.stop()\n",
    "                        time.sleep(60)\n",
    "                        c_service.start()\n",
    "                        browser = webdriver.Chrome(chrome_options=chrome_options)  #调用Chrome浏览器\n",
    "                        browser.get(\"https://archiveofourown.org/\")\n",
    "                        time.sleep(5)\n",
    "                        browser.find_element_by_id('tos_agree').click()\n",
    "                        time.sleep(2)\n",
    "                        browser.find_element_by_id('accept_tos').click()\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        browser.get(work_url)\n",
    "\n",
    "                        work_html_text = browser.page_source  #获得页面代码\n",
    "\n",
    "                        if \"If you accept cookies from our site and you choose \\\"Proceed\\\"\" in work_html_text: #无法获取正文则点击Proceed\n",
    "                            browser.find_element_by_link_text('Proceed').click()\n",
    "                            time.sleep(1)\n",
    "                            browser.get(work_url)\n",
    "                            work_html_text = browser.page_source\n",
    "\n",
    "\n",
    "                if \"<!--chapter content-->\" in work_html_text:\n",
    "\n",
    "                    work_name = work_url.replace('https://archiveofourown','').replace('.com/','').replace('.org/','').replace('/','_').replace(\".\",\"_\")\n",
    "                    work_path = os.path.join(dir_path, work_name+\".html\")\n",
    "                    print(work_path)\n",
    "                    fh = open(work_path, 'w') #保存页面\n",
    "                    fh.write(work_html_text) #写入内容\n",
    "                    fh.close() #关闭\n",
    "                time.sleep(float(random.randint(10,50))/10) #随机延时\n",
    "    return browser\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_p = 0\n",
    "end_p = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiwei/miniconda3/envs/dslen/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: use options instead of chrome_options\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "c_service = webdriver.chrome.service.Service('/usr/bin/chromedriver')\n",
    "c_service.command_line_args()\n",
    "c_service.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=socks5://localhost:1080')\n",
    "\n",
    "browser = webdriver.Chrome(chrome_options=chrome_options)  # 调用Chrome浏览器\n",
    "for page in range(start_p, end_p):\n",
    "    \n",
    "        google_search_url = make_google_search_url(page)\n",
    "        browser.get(google_search_url)\n",
    "\n",
    "        html_text = browser.page_source  # 获得页面代码\n",
    "\n",
    "        lofter_list = get_url_from_search(html_text)\n",
    "        browser = find_ao3_from_lofter(lofter_list, browser, \"lofter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mylist = get_url_from_search(html_text)\n",
    "print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取搜索页面\n",
    "def make_search_url(page=1, langu=\"zh\", rating_key=\"\"):\n",
    "    rating = {\n",
    "    \"\": \"\",\n",
    "    \"Not_Rated\": 9,\n",
    "    \"General_Audiences\": 10, #一般观众\n",
    "    \"Teen_And_Up_Audiences\": 11, #青少年及以上观众\n",
    "    \"Mature\": 12, #成熟\n",
    "    \"Explicit\": 13, #明确的\n",
    "    }\n",
    "\n",
    "    base_loc = 'https://archiveofourown.org/works/'\n",
    "\n",
    "    base_loc += \"search?commit=Search&page=\"+str(page)+\"&utf8=%E2%9C%93\" #搜索页\n",
    "    base_loc += \"&work_search%5Bbookmarks_count%5D=\"\n",
    "    base_loc += \"&work_search%5Bcharacter_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bcomments_count%5D=\"\n",
    "    base_loc += \"&work_search%5Bcomplete%5D=\"\n",
    "    base_loc += \"&work_search%5Bcreators%5D=\"\n",
    "    base_loc += \"&work_search%5Bcrossover%5D=\"\n",
    "    base_loc += \"&work_search%5Bfandom_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bfreeform_names%5D=\"\n",
    "    base_loc += \"&work_search%5Bhits%5D=\"\n",
    "    base_loc += \"&work_search%5Bkudos_count%5D=\"\n",
    "    base_loc += \"&work_search%5Blanguage_id%5D=\" + langu #语言\n",
    "    base_loc += \"&work_search%5Bquery%5D=\"\n",
    "    base_loc += \"&work_search%5Brating_ids%5D=\" + rating[rating_key] #分级\n",
    "    base_loc += \"&work_search%5Brelationship_names%5D=\"\n",
    "    base_loc += \"&work_search%5Brevised_at%5D=\"\n",
    "    base_loc += \"&work_search%5Bsingle_chapter%5D=0\"\n",
    "    base_loc += \"&work_search%5Bsort_column%5D=_score\"\n",
    "    base_loc += \"&work_search%5Bsort_direction%5D=desc\"\n",
    "    base_loc += \"&work_search%5Btitle%5D=\"\n",
    "    base_loc += \"&work_search%5Bword_count%5D=\"\n",
    "\n",
    "    return base_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看看搜索页面的html,在chrome中可以按F12打开开发者工具。 `Ctrl+Shift+C` 使用元素选择工具点击一下文章标题，查看后发现所有的搜索结果都在 `<ol class=work index group>`标签下，并且在`li`标签的`id`中记录了文章的id。\n",
    "![页面分析](./img/worklish.png)\n",
    "我们点击进入一篇文章，查看文章的url发现文章url与上面的id是一一对应的。这样，我们就可以通过分析搜索页得到文章的地址。\n",
    "![文章页面](./img/workurl.png)\n",
    "通过BeautifulSoup抓取相应标签获得`li`标签的`id`,就可以得到该搜索页下面所有的文章地址了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取文章链接\n",
    "def get_work_id_from_search(html):\n",
    "    old_list = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ol = soup.find('ol', attrs={'class': 'work index group'})\n",
    "    work_blurb_groups = ol.findAll('li', attrs={'class': 'work blurb group'})\n",
    "    for wbg in work_blurb_groups:\n",
    "        if wbg[\"id\"] not in old_list:\n",
    "            old_list.append(wbg[\"id\"])\n",
    "    \n",
    "    return old_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做好这些准备工作下面我们就开始正式爬取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fulltext/\" #存储文章的文件夹\n",
    "\n",
    "#5000页中文内容,这里可以先取较小的数字做测试\n",
    "start_p = 1\n",
    "end_p = 5000\n",
    "\n",
    "pbar = tqdm(range(start_p, end_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体思路是这样的：\n",
    "1. 打开一个浏览器，需要注意的我这里使用了代理，否则无法浏览到AO3；\n",
    "2. 通过selenium `find_element_by_id` 功能找到相应按钮自动点击，同意网站条款；\n",
    "3. 进入循环，通过`make_search_url`函数组合出搜索页的链接，遍历页码；\n",
    "4. 将搜索页的html传入函数`get_work_id_from_search`提取出所有文章id；\n",
    "5. 遍历文章id通过文章id组合出文章地址并访问，最后保存文章页面的html。\n",
    "\n",
    "这其中有两个注意事项：\n",
    "1. 当进入限制级文章时，网站会提示再次同意浏览条款，当检测到条款关键字时，使用`find_element_by_link_text('Proceed').click()`点击确认即可；\n",
    "2. 频繁访问后，网站会拒绝访问请求出现‘Retry later’页面，当检测到这种情况后，进行异常处理，关闭当前的浏览器，等待一分钟后重新访问。（这也是爬取文章速度比较慢的原因，有大神知道怎么解决的请赐教）\n",
    "\n",
    "这里声明一下，我是不会教大家如何挂代理的，这个本身就是我自己专用的，私下要也不会给的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_service = webdriver.chrome.service.Service('/usr/bin/chromedriver')\n",
    "c_service.command_line_args()\n",
    "c_service.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=socks5://localhost:1080')\n",
    "\n",
    "browser = webdriver.Chrome(chrome_options=chrome_options)  # 调用Chrome浏览器\n",
    "\n",
    "browser.get(\"https://archiveofourown.org/\")\n",
    "\n",
    "time.sleep(3)\n",
    "browser.find_element_by_id('tos_agree').click()\n",
    "time.sleep(1)\n",
    "browser.find_element_by_id('accept_tos').click()\n",
    "time.sleep(1)\n",
    "\n",
    "for page in pbar:\n",
    "    search_url = make_search_url(page) #生成搜寻页面链接\n",
    "    browser.get(search_url)  # 请求页面，打开一个浏览器\n",
    "\n",
    "    html_text = browser.page_source  # 获得页面代码\n",
    "    try:\n",
    "        work_list = get_work_id_from_search(html_text) #获得文章的id\n",
    "        for work in work_list:\n",
    "            work_path = os.path.join(save_path, work+\".html\")\n",
    "\n",
    "            if os.path.exists(work_path):\n",
    "                continue\n",
    "            work_url = \"https://archiveofourown.org/works/\" + work.split(\"_\")[1] #创建文章URL\n",
    "            browser.get(work_url)\n",
    "            \n",
    "            html_text = browser.page_source  #获得页面代码\n",
    "            if \"If you accept cookies from our site and you choose \\\"Proceed\\\"\" in html_text: #无法获取正文则点击Proceed\n",
    "                browser.find_element_by_link_text('Proceed').click()\n",
    "                time.sleep(1)\n",
    "                browser.get(work_url)\n",
    "                html_text = browser.page_source\n",
    "\n",
    "            if \"Retry later\" in html_text:\n",
    "                raise AttributeError\n",
    "            if \"<!--chapter content-->\" in html_text:\n",
    "                pbar.set_description(\"saving: \" + work)\n",
    "                fh = open(work_path, 'w') #保存页面\n",
    "                fh.write(html_text) #写入内容\n",
    "                fh.close() #关闭\n",
    "            time.sleep(float(random.randint(10,50))/10) #随机延时\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        time.sleep(3)\n",
    "        browser.get(\"http://www.baidu.com\")\n",
    "        time.sleep(3)\n",
    "        browser.quit()\n",
    "        c_service.stop()\n",
    "        time.sleep(60)\n",
    "        c_service.start()\n",
    "        browser = webdriver.Chrome(chrome_options=chrome_options)  #调用Chrome浏览器\n",
    "        browser.get(\"https://archiveofourown.org/\")\n",
    "        time.sleep(5)\n",
    "        browser.find_element_by_id('tos_agree').click()\n",
    "        time.sleep(2)\n",
    "        browser.find_element_by_id('accept_tos').click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "    time.sleep(float(random.randint(10,50))/10) #随机延时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
