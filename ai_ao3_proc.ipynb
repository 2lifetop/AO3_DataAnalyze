{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取词表\n",
    "def read_words_list(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    strlist = []\n",
    "    for l in lines:\n",
    "        if '#' != l[0] and '' != l.strip():\n",
    "            l = l.strip()\n",
    "            strlist.append(l)\n",
    "    return strlist\n",
    "\n",
    "#去除常用词\n",
    "def remove_stop_words(text, stop_words):\n",
    "    #保存过滤词数量的字典\n",
    "    swords_cnt = {}\n",
    "        \n",
    "    while \"  \" in text: #去掉多余空格\n",
    "        text = text.replace('  ', ' ')\n",
    "    for key, words in stop_words.items():\n",
    "        swords_cnt[key] = np.zeros(len(words)) #创建向量\n",
    "        for i,stpwd in enumerate(words):\n",
    "            if (stpwd) in text:\n",
    "                text = text.replace(' '+stpwd+' ', ' ')\n",
    "#                 swords_cnt[key][i] += text.count(stpwd)\n",
    "                swords_cnt[key][i] += 1\n",
    "    return text, swords_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence(sentence):\n",
    "    return [token for token in jieba.lcut(sentence) if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = 'hit_stopwords.txt'\n",
    "stop_words = read_words_list(stop_words_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize=cut_sentence，sequence=True表示输入的是一个sequence类型的数据\n",
    "ao3_text = torchtext.data.Field(sequential=True,lower=True,tokenize=cut_sentence, fix_length=200)\n",
    "#LabelField对象，sequential=False，标签不是dtype=torch.int64标签转化成整形\n",
    "ao3_label = torchtext.data.LabelField(sequential=False, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.617 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#这里主要是告诉torchtext需要处理哪些数据，这些数据存放在哪里，TabularDataset是一个处理scv/tsv的常用类\n",
    "train_dataset,test_dataset = torchtext.data.TabularDataset.splits(\n",
    "      path='train',  #文件存放路径\n",
    "      format='csv',   #文件格式\n",
    "      skip_header=False,  #不跳过表头\n",
    "      train='train.csv',  \n",
    "      test='test.csv',    \n",
    "      fields=[('label',ao3_label),('content',ao3_text)] # 定义数据对应的表头\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = 'sgns.sogou.word' # 预训练词向量文件名\n",
    "pretrained_path = './word_embedding' #预训练词向量存放路径\n",
    "vectors = torchtext.vocab.Vectors(name=pretrained_name, cache=pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao3_text.build_vocab(train_dataset, test_dataset, vectors=vectors)\n",
    "ao3_label.build_vocab(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.data.BucketIterator.splits \n",
    "#BucketIterator能将样本长度接近的句子尽量放在同一个batch里面\n",
    "#句子长度差距过大，就会给短句加入过多的无意义的<pad>\n",
    "#句子长度相近的在一个batch里面的话，能够避免这个问题\n",
    "train_iter,test_iter = torchtext.data.BucketIterator.splits(\n",
    "        (train_dataset, test_dataset),#需要生成迭代器的数据集\n",
    "        batch_sizes=(128, 128), # 每个迭代器分别以多少样本为一个batch\n",
    "        sort_key=lambda x: len(x.content) #按什么顺序来排列batch，这里是以句子的长度，就是上面说的把句子长度相近的放在同一个batch里面\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextCNN 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,class_num, # 分类数\n",
    "                 filter_sizes, # 卷积核的长也就是滑动窗口的长 \n",
    "                 filter_num,   # 卷积核的数量 \n",
    "                 vocabulary_size, # 词表的大小\n",
    "                 embedding_dimension, # 词向量的维度\n",
    "                 vectors, # 词向量\n",
    "                 dropout): # dropout率\n",
    "        super(TextCNN, self).__init__() # 继承nn.Module\n",
    "\n",
    "        chanel_num = 1  # 通道数，也就是一篇文章一个样本只相当于一个feature map\n",
    "\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # 嵌入层 \n",
    "        self.embedding = self.embedding.from_pretrained(vectors) #嵌入层加载预训练词向量\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(chanel_num, filter_num, (fsz, embedding_dimension)) for fsz in filter_sizes])  # 卷积层\n",
    "        self.dropout = nn.Dropout(dropout) # dropout\n",
    "        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num) #全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x维度[句子长度,一个batch中所包含的样本数] 例:[3451,128]\n",
    "        x = self.embedding(x) # #经过嵌入层之后x的维度，[句子长度,一个batch中所包含的样本数,词向量维度] 例：[3451,128,300]\n",
    "        x = x.permute(1,0,2) # permute函数将样本数和句子长度换一下位置，[一个batch中所包含的样本数,句子长度,词向量维度] 例：[128,3451,300]\n",
    "        x = x.unsqueeze(1) # # conv2d需要输入的是一个四维数据，所以新增一维feature map数 unsqueeze(1)表示在第一维处新增一维，[一个batch中所包含的样本数,一个样本中的feature map数，句子长度,词向量维度] 例：[128,1,3451,300]\n",
    "        x = [conv(x) for conv in self.convs] # 与卷积核进行卷积，输出是[一个batch中所包含的样本数,卷积核数，句子长度-卷积核size+1,1]维数据,因为有[3,4,5]三张size类型的卷积核所以用列表表达式 例：[[128,16,3459,1],[128,16,3458,1],[128,16,3457,1]]\n",
    "        x = [sub_x.squeeze(3) for sub_x in x]#squeeze(3)判断第三维是否是1，如果是则压缩，如不是则保持原样 例：[[128,16,3459],[128,16,3458],[128,16,3457]]\n",
    "        x = [F.relu(sub_x) for sub_x in x] # ReLU激活函数激活，不改变x维度 \n",
    "        x = [F.max_pool1d(sub_x,sub_x.size(2)) for sub_x in x] # 池化层，根据之前说的原理，max_pool1d要取出每一个滑动窗口生成的矩阵的最大值，因此在第二维上取最大值 例：[[128,16,1],[128,16,1],[128,16,1]]\n",
    "        x = [sub_x.squeeze(2) for sub_x in x] # 判断第二维是否为1，若是则压缩 例：[[128,16],[128,16],[128,16]]\n",
    "        x = torch.cat(x, 1) # 进行拼接，例：[128,48]\n",
    "        x = self.dropout(x) # 去除掉一些神经元防止过拟合，注意dropout之后x的维度依旧是[128,48]，并不是说我dropout的概率是0.5，去除了一半的神经元维度就变成了[128,24]，而是把x中的一些神经元的数据根据概率全部变成了0，维度依旧是[128,48]\n",
    "        logits = self.fc(x) # 全接连层 例：输入x是[128,48] 输出logits是[128,10]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(ao3_label.vocab) # 类别数目\n",
    "filter_size = [3,4,5]  # 卷积核种类数 \n",
    "filter_num=16   # 卷积核数量\n",
    "vocab_size = len(ao3_text.vocab) # 词表大小\n",
    "embedding_dim = ao3_text.vocab.vectors.size()[-1] # 词向量维度\n",
    "vectors = ao3_text.vocab.vectors # 词向量\n",
    "dropout=0.5 \n",
    "learning_rate = 0.001  # 学习率\n",
    "epochs = 5   # 迭代次数\n",
    "save_dir = './model' # 模型保存路径\n",
    "steps_show = 10   # 每10步查看一次训练集loss和mini batch里的准确率\n",
    "steps_eval = 200  # 每100步测试一下验证集的准确率\n",
    "early_stopping = 1000  # 若发现当前验证集的准确率在1000步训练之后不再提高 一直小于best_acc,则提前停止训练\n",
    "\n",
    "textcnn_model = TextCNN(class_num=class_num,\n",
    "        filter_sizes=filter_size,\n",
    "        filter_num=filter_num,\n",
    "        vocabulary_size=vocab_size,\n",
    "        embedding_dimension=embedding_dim,\n",
    "        vectors=vectors,\n",
    "        dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义train函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, dev_iter, model):\n",
    "\n",
    "    if torch.cuda.is_available(): # 判断是否有GPU，如果有把模型放在GPU上训练，速度质的飞跃\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 梯度下降优化器，采用Adam\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1): \n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.content, batch.label\n",
    "            if torch.cuda.is_available(): # 如果有GPU将特征更新放在GPU上\n",
    "                  feature,target = feature.cuda(),target.cuda() \n",
    "            optimizer.zero_grad() # 将梯度初始化为0，每个batch都是独立训练地，因为每训练一个batch都需要将梯度归零\n",
    "            logits = model(feature)\n",
    "            loss = F.cross_entropy(logits, target) # 计算损失函数 采用交叉熵损失函数\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step() # 放在loss.backward()后进行参数的更新\n",
    "            steps += 1 \n",
    "            if steps % steps_show == 0: # 每训练多少步计算一次准确率，我这边是1，可以自己修改\n",
    "                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum() # logits是[128,10],torch.max(logits, 1)也就是选出第一维中概率最大的值，输出为[128,1],torch.max(logits, 1)[1]相当于把每一个样本的预测输出取出来，然后通过view(target.size())平铺成和target一样的size (128,),然后把与target中相同的求和，统计预测正确的数量\n",
    "                train_acc = 100.0 * corrects / batch.batch_size # 计算每个mini batch中的准确率\n",
    "                print('steps:{} - loss: {:.6f}  acc:{:.4f}'.format(\n",
    "                  steps,\n",
    "                  loss.item(),\n",
    "                  train_acc))\n",
    "                \n",
    "            if steps % steps_eval == 0: # 每训练steps_eval步进行一次验证\n",
    "#                 save(model,save_dir, steps)\n",
    "                dev_acc = dev_eval(dev_iter,model)\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    last_step = steps\n",
    "                    print('Saving best model, acc: {:.4f}%\\n'.format(best_acc))\n",
    "                    save(model,save_dir, steps)\n",
    "                else:\n",
    "                    if steps - last_step >= early_stopping:\n",
    "                        print('\\n提前停止于 {} steps, acc: {:.4f}%'.format(last_step, best_acc))\n",
    "                        raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_eval(dev_iter,model):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in dev_iter:\n",
    "        feature, target = batch.content, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "        logits = model(feature)\n",
    "        sorc = torch.argmax(logits,dim=1)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                    [1].view(target.size()).data == target.data).sum()\n",
    "    size = len(dev_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                      accuracy,\n",
    "                                                                      corrects,\n",
    "                                                                      size))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型保存函数\n",
    "def save(model, save_dir, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = 'bestmodel_steps{}.pt'.format(steps)\n",
    "    save_bestmodel_path = os.path.join(save_dir, save_path)\n",
    "    torch.save(model.state_dict(), save_bestmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:10 - loss: 0.288333  acc:92.1875\n",
      "steps:20 - loss: 0.188884  acc:92.9688\n",
      "steps:30 - loss: 0.200837  acc:92.9688\n",
      "steps:40 - loss: 0.159954  acc:95.3125\n",
      "steps:50 - loss: 0.225896  acc:92.1875\n",
      "steps:60 - loss: 0.228430  acc:92.9688\n",
      "steps:70 - loss: 0.123649  acc:95.3125\n",
      "steps:80 - loss: 0.128664  acc:92.9688\n",
      "steps:90 - loss: 0.082676  acc:95.3125\n",
      "steps:100 - loss: 0.124748  acc:93.7500\n",
      "steps:110 - loss: 0.071438  acc:96.0938\n",
      "steps:120 - loss: 0.102413  acc:95.3125\n",
      "steps:130 - loss: 0.086127  acc:96.8750\n",
      "steps:140 - loss: 0.155415  acc:92.9688\n",
      "steps:150 - loss: 0.090518  acc:96.8750\n",
      "steps:160 - loss: 0.044919  acc:98.4375\n",
      "steps:170 - loss: 0.127971  acc:96.8750\n",
      "steps:180 - loss: 0.138537  acc:95.3125\n",
      "steps:190 - loss: 0.101599  acc:96.8750\n",
      "steps:200 - loss: 0.060814  acc:97.6562\n",
      "\n",
      "Evaluation - loss: 0.000696  acc: 96.6422%(2504/2591) \n",
      "\n",
      "Saving best model, acc: 96.6422%\n",
      "\n",
      "steps:210 - loss: 0.032192  acc:99.2188\n",
      "steps:220 - loss: 0.046644  acc:97.6562\n",
      "steps:230 - loss: 0.023588  acc:99.2188\n",
      "steps:240 - loss: 0.021109  acc:98.4375\n",
      "steps:250 - loss: 0.029260  acc:100.0000\n",
      "steps:260 - loss: 0.014136  acc:99.2188\n",
      "steps:270 - loss: 0.015079  acc:100.0000\n",
      "steps:280 - loss: 0.013254  acc:100.0000\n",
      "steps:290 - loss: 0.036183  acc:99.2188\n",
      "steps:300 - loss: 0.019952  acc:100.0000\n",
      "steps:310 - loss: 0.013352  acc:99.2188\n",
      "steps:320 - loss: 0.039715  acc:98.4375\n",
      "steps:330 - loss: 0.016906  acc:99.2188\n",
      "steps:340 - loss: 0.040419  acc:98.4375\n",
      "steps:350 - loss: 0.030449  acc:98.4375\n",
      "steps:360 - loss: 0.041432  acc:97.6562\n",
      "steps:370 - loss: 0.010923  acc:100.0000\n",
      "steps:380 - loss: 0.026274  acc:98.4375\n",
      "steps:390 - loss: 0.006294  acc:100.0000\n",
      "steps:400 - loss: 0.024468  acc:99.2188\n",
      "\n",
      "Evaluation - loss: 0.000649  acc: 96.7580%(2507/2591) \n",
      "\n",
      "Saving best model, acc: 96.7580%\n",
      "\n",
      "steps:410 - loss: 0.011676  acc:100.0000\n",
      "steps:420 - loss: 0.018096  acc:99.2188\n",
      "steps:430 - loss: 0.016769  acc:99.2188\n",
      "steps:440 - loss: 0.035536  acc:99.2188\n",
      "steps:450 - loss: 0.026959  acc:99.2188\n",
      "steps:460 - loss: 0.007493  acc:100.0000\n",
      "steps:470 - loss: 0.035576  acc:98.4375\n",
      "steps:480 - loss: 0.028116  acc:98.4375\n",
      "steps:490 - loss: 0.053880  acc:98.4375\n",
      "steps:500 - loss: 0.002628  acc:100.0000\n",
      "steps:510 - loss: 0.030826  acc:98.4375\n",
      "steps:520 - loss: 0.026302  acc:100.0000\n",
      "steps:530 - loss: 0.014514  acc:99.2188\n",
      "steps:540 - loss: 0.059949  acc:96.8750\n",
      "steps:550 - loss: 0.041698  acc:98.4375\n",
      "steps:560 - loss: 0.010040  acc:100.0000\n",
      "steps:570 - loss: 0.021564  acc:99.2188\n",
      "steps:580 - loss: 0.011322  acc:100.0000\n",
      "steps:590 - loss: 0.032791  acc:98.4375\n",
      "steps:600 - loss: 0.015923  acc:100.0000\n",
      "\n",
      "Evaluation - loss: 0.000631  acc: 96.9896%(2513/2591) \n",
      "\n",
      "Saving best model, acc: 96.9896%\n",
      "\n",
      "steps:610 - loss: 0.005118  acc:100.0000\n",
      "steps:620 - loss: 0.005453  acc:100.0000\n",
      "steps:630 - loss: 0.008191  acc:100.0000\n",
      "steps:640 - loss: 0.035210  acc:98.4375\n",
      "steps:650 - loss: 0.011846  acc:100.0000\n",
      "steps:660 - loss: 0.027770  acc:97.6562\n",
      "steps:670 - loss: 0.013032  acc:99.2188\n",
      "steps:680 - loss: 0.008512  acc:100.0000\n",
      "steps:690 - loss: 0.014073  acc:100.0000\n",
      "steps:700 - loss: 0.015440  acc:99.2188\n",
      "steps:710 - loss: 0.021132  acc:98.4375\n",
      "steps:720 - loss: 0.012799  acc:100.0000\n",
      "steps:730 - loss: 0.044788  acc:98.4375\n",
      "steps:740 - loss: 0.015589  acc:100.0000\n",
      "steps:750 - loss: 0.047585  acc:97.6562\n",
      "steps:760 - loss: 0.011398  acc:100.0000\n",
      "steps:770 - loss: 0.018346  acc:99.2188\n",
      "steps:780 - loss: 0.008383  acc:100.0000\n",
      "steps:790 - loss: 0.032846  acc:99.2188\n",
      "steps:800 - loss: 0.026798  acc:98.4375\n",
      "\n",
      "Evaluation - loss: 0.000633  acc: 97.4527%(2525/2591) \n",
      "\n",
      "Saving best model, acc: 97.4527%\n",
      "\n",
      "steps:810 - loss: 0.012831  acc:100.0000\n",
      "steps:820 - loss: 0.005983  acc:100.0000\n",
      "steps:830 - loss: 0.013559  acc:100.0000\n",
      "steps:840 - loss: 0.018223  acc:100.0000\n",
      "steps:850 - loss: 0.009725  acc:100.0000\n",
      "steps:860 - loss: 0.010541  acc:100.0000\n",
      "steps:870 - loss: 0.030457  acc:99.2188\n",
      "steps:880 - loss: 0.009395  acc:100.0000\n",
      "steps:890 - loss: 0.027197  acc:99.2188\n",
      "steps:900 - loss: 0.013630  acc:100.0000\n",
      "steps:910 - loss: 0.009400  acc:100.0000\n",
      "steps:920 - loss: 0.018897  acc:100.0000\n",
      "steps:930 - loss: 0.009530  acc:100.0000\n",
      "steps:940 - loss: 0.010002  acc:100.0000\n",
      "steps:950 - loss: 0.010487  acc:100.0000\n",
      "steps:960 - loss: 0.021958  acc:99.2188\n",
      "steps:970 - loss: 0.019367  acc:99.2188\n",
      "steps:980 - loss: 0.012731  acc:100.0000\n",
      "steps:990 - loss: 0.008297  acc:100.0000\n",
      "steps:1000 - loss: 0.004086  acc:100.0000\n",
      "\n",
      "Evaluation - loss: 0.000658  acc: 97.1826%(2518/2591) \n",
      "\n",
      "steps:1010 - loss: 0.016688  acc:99.2188\n",
      "steps:1020 - loss: 0.014281  acc:100.0000\n",
      "steps:1030 - loss: 0.028814  acc:98.4375\n",
      "steps:1040 - loss: 0.012530  acc:100.0000\n",
      "steps:1050 - loss: 0.026766  acc:99.2188\n",
      "steps:1060 - loss: 0.005349  acc:100.0000\n",
      "steps:1070 - loss: 0.020534  acc:99.2188\n",
      "steps:1080 - loss: 0.012576  acc:100.0000\n",
      "steps:1090 - loss: 0.005431  acc:100.0000\n",
      "steps:1100 - loss: 0.016850  acc:100.0000\n",
      "steps:1110 - loss: 0.008640  acc:100.0000\n",
      "steps:1120 - loss: 0.012661  acc:100.0000\n",
      "steps:1130 - loss: 0.008095  acc:100.0000\n",
      "steps:1140 - loss: 0.014021  acc:100.0000\n",
      "steps:1150 - loss: 0.018610  acc:99.2188\n",
      "steps:1160 - loss: 0.008785  acc:100.0000\n",
      "steps:1170 - loss: 0.005656  acc:100.0000\n",
      "steps:1180 - loss: 0.008731  acc:100.0000\n",
      "steps:1190 - loss: 0.013803  acc:100.0000\n",
      "steps:1200 - loss: 0.016609  acc:99.2188\n",
      "\n",
      "Evaluation - loss: 0.000649  acc: 97.3755%(2523/2591) \n",
      "\n",
      "steps:1210 - loss: 0.013762  acc:100.0000\n",
      "steps:1220 - loss: 0.017824  acc:100.0000\n",
      "steps:1230 - loss: 0.009749  acc:100.0000\n",
      "steps:1240 - loss: 0.013434  acc:99.2188\n",
      "steps:1250 - loss: 0.003960  acc:100.0000\n",
      "steps:1260 - loss: 0.009735  acc:100.0000\n",
      "steps:1270 - loss: 0.013335  acc:100.0000\n",
      "steps:1280 - loss: 0.010105  acc:100.0000\n",
      "steps:1290 - loss: 0.004628  acc:100.0000\n",
      "steps:1300 - loss: 0.012502  acc:100.0000\n",
      "steps:1310 - loss: 0.009116  acc:100.0000\n",
      "steps:1320 - loss: 0.013638  acc:99.2188\n",
      "steps:1330 - loss: 0.010473  acc:100.0000\n",
      "steps:1340 - loss: 0.006555  acc:100.0000\n",
      "steps:1350 - loss: 0.003504  acc:100.0000\n",
      "steps:1360 - loss: 0.011896  acc:100.0000\n",
      "steps:1370 - loss: 0.005947  acc:100.0000\n",
      "steps:1380 - loss: 0.024309  acc:99.2188\n",
      "steps:1390 - loss: 0.004256  acc:100.0000\n",
      "steps:1400 - loss: 0.012854  acc:100.0000\n",
      "\n",
      "Evaluation - loss: 0.000683  acc: 97.3755%(2523/2591) \n",
      "\n",
      "steps:1410 - loss: 0.002781  acc:100.0000\n",
      "steps:1420 - loss: 0.006518  acc:100.0000\n",
      "steps:1430 - loss: 0.004089  acc:100.0000\n",
      "steps:1440 - loss: 0.009407  acc:100.0000\n",
      "steps:1450 - loss: 0.013953  acc:100.0000\n",
      "steps:1460 - loss: 0.004821  acc:100.0000\n",
      "steps:1470 - loss: 0.003619  acc:100.0000\n",
      "steps:1480 - loss: 0.007762  acc:100.0000\n",
      "steps:1490 - loss: 0.018251  acc:99.2188\n",
      "steps:1500 - loss: 0.007503  acc:100.0000\n",
      "steps:1510 - loss: 0.005853  acc:100.0000\n",
      "steps:1520 - loss: 0.005509  acc:100.0000\n",
      "steps:1530 - loss: 0.005901  acc:100.0000\n",
      "steps:1540 - loss: 0.007868  acc:100.0000\n",
      "steps:1550 - loss: 0.005797  acc:100.0000\n",
      "steps:1560 - loss: 0.004663  acc:100.0000\n",
      "steps:1570 - loss: 0.008696  acc:100.0000\n",
      "steps:1580 - loss: 0.007104  acc:100.0000\n",
      "steps:1590 - loss: 0.012517  acc:99.2188\n",
      "steps:1600 - loss: 0.011276  acc:99.2188\n",
      "\n",
      "Evaluation - loss: 0.000660  acc: 97.4141%(2524/2591) \n",
      "\n",
      "steps:1610 - loss: 0.006550  acc:100.0000\n",
      "steps:1620 - loss: 0.010389  acc:100.0000\n",
      "steps:1630 - loss: 0.017661  acc:99.2188\n",
      "steps:1640 - loss: 0.010734  acc:100.0000\n",
      "steps:1650 - loss: 0.008496  acc:100.0000\n",
      "steps:1660 - loss: 0.009451  acc:100.0000\n",
      "steps:1670 - loss: 0.008520  acc:100.0000\n",
      "steps:1680 - loss: 0.021540  acc:99.2188\n",
      "steps:1690 - loss: 0.009913  acc:100.0000\n",
      "steps:1700 - loss: 0.004056  acc:100.0000\n",
      "steps:1710 - loss: 0.004387  acc:100.0000\n",
      "steps:1720 - loss: 0.004284  acc:100.0000\n",
      "steps:1730 - loss: 0.013641  acc:100.0000\n",
      "steps:1740 - loss: 0.004610  acc:100.0000\n",
      "steps:1750 - loss: 0.007736  acc:100.0000\n",
      "steps:1760 - loss: 0.007015  acc:100.0000\n",
      "steps:1770 - loss: 0.012292  acc:100.0000\n",
      "steps:1780 - loss: 0.020330  acc:99.2188\n",
      "steps:1790 - loss: 0.020706  acc:99.2188\n",
      "steps:1800 - loss: 0.006482  acc:100.0000\n",
      "\n",
      "Evaluation - loss: 0.000702  acc: 97.2983%(2521/2591) \n",
      "\n",
      "\n",
      "提前停止于 800 steps, acc: 97.4527%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ea034028e1c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#开始训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtextcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-cf0d7f5d2009>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, dev_iter, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n提前停止于 {} steps, acc: {:.4f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "train(train_iter,test_iter,textcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.000702  acc: 97.2983%(2521/2591) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(97.2983)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_eval(test_iter,textcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field_path = \"./word_embedding/ao3_text.field\"\n",
    "label_field_path = \"./word_embedding/ao3_label.field\"\n",
    "torch.save(ao3_text ,text_field_path)\n",
    "torch.save(ao3_label ,label_field_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
